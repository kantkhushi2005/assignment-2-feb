{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dc4734b-33d3-47df-ab9f-501b374d5d55",
   "metadata": {},
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "\n",
    "To find the probability that an employee is a smoker given that they use the company’s health insurance plan, we use the conditional probability formula. Given:\n",
    "\n",
    "- \\( P(\\text{Uses Plan}) = 0.70 \\) (70% of employees use the health insurance plan)\n",
    "- \\( P(\\text{Smoker} \\mid \\text{Uses Plan}) = 0.40 \\) (40% of those who use the plan are smokers)\n",
    "\n",
    "We want to find \\( P(\\text{Smoker} \\mid \\text{Uses Plan}) \\), which is already provided directly as \\( 0.40 \\).\n",
    "\n",
    "So, the probability that an employee is a smoker given that they use the health insurance plan is \\( \\boxed{0.40} \\) or 40%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b47b69f-586a-4704-88a9-817085b52c45",
   "metadata": {},
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "\n",
    "**Bernoulli Naive Bayes** and **Multinomial Naive Bayes** are both types of Naive Bayes classifiers used for different types of data. Here’s a concise comparison:\n",
    "\n",
    "### Bernoulli Naive Bayes\n",
    "\n",
    "- **Feature Type:** Binary/Boolean features.\n",
    "- **Assumption:** Each feature is binary (0 or 1), indicating the presence or absence of a feature.\n",
    "- **Application:** Suitable for text classification where features represent the presence or absence of words.\n",
    "- **Example:** Classifying emails as spam or not spam based on whether certain keywords are present or absent.\n",
    "\n",
    "**Formula:**\n",
    "\\[\n",
    "P(\\text{Class} \\mid \\text{Features}) = \\frac{P(\\text{Class}) \\cdot \\prod_{i=1}^{n} P(\\text{Feature}_i \\mid \\text{Class})^{\\text{Feature}_i}}{P(\\text{Features})}\n",
    "\\]\n",
    "where \\(\\text{Feature}_i\\) is either 0 (absent) or 1 (present).\n",
    "\n",
    "### Multinomial Naive Bayes\n",
    "\n",
    "- **Feature Type:** Count-based or frequency-based features.\n",
    "- **Assumption:** Features represent counts or frequencies of events (e.g., word counts).\n",
    "- **Application:** Suitable for text classification where features are term frequencies or counts.\n",
    "- **Example:** Classifying documents based on the frequency of words.\n",
    "\n",
    "**Formula:**\n",
    "\\[\n",
    "P(\\text{Class} \\mid \\text{Features}) = \\frac{P(\\text{Class}) \\cdot \\prod_{i=1}^{n} \\frac{P(\\text{Feature}_i \\mid \\text{Class})^{\\text{Feature}_i}}{\\text{Feature}_i!}}{P(\\text{Features})}\n",
    "\\]\n",
    "where \\(\\text{Feature}_i\\) represents the count or frequency of the \\(i\\)-th feature.\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "1. **Feature Representation:**\n",
    "   - **Bernoulli Naive Bayes:** Features are binary (presence/absence).\n",
    "   - **Multinomial Naive Bayes:** Features are counts or frequencies.\n",
    "\n",
    "2. **Model Assumptions:**\n",
    "   - **Bernoulli Naive Bayes:** Assumes each feature is a binary indicator.\n",
    "   - **Multinomial Naive Bayes:** Assumes features follow a multinomial distribution (counts of events).\n",
    "\n",
    "3. **Use Cases:**\n",
    "   - **Bernoulli Naive Bayes:** Suitable for datasets where features are binary.\n",
    "   - **Multinomial Naive Bayes:** Suitable for datasets where features are counts or frequencies, often used in text classification.\n",
    "\n",
    "Choosing between these models depends on the nature of your features and the specific characteristics of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84db7c7a-9b17-4681-b0f6-18de865b3c62",
   "metadata": {},
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "Bernoulli Naive Bayes does not inherently handle missing values directly. If your dataset has missing values, you need to address them before applying Bernoulli Naive Bayes. Here are some common strategies for handling missing values:\n",
    "\n",
    "1. **Imputation:**\n",
    "   - **Method:** Replace missing values with a default value, such as the most frequent value (mode), a specific constant, or the mean/median of the non-missing values.\n",
    "   - **Example:** For binary features, you might replace missing values with the most common value (0 or 1).\n",
    "\n",
    "2. **Removing Data:**\n",
    "   - **Method:** Remove rows or columns with missing values. This approach is straightforward but can lead to loss of data.\n",
    "   - **Example:** If a feature has many missing values, you might discard that feature or drop rows with missing values.\n",
    "\n",
    "3. **Model-Based Imputation:**\n",
    "   - **Method:** Use other models (e.g., regression) to predict and fill in missing values based on other features.\n",
    "   - **Example:** Predict missing feature values using a regression model trained on non-missing data.\n",
    "\n",
    "4. **Indicator Variable:**\n",
    "   - **Method:** Create an additional binary feature indicating whether the original feature had missing values.\n",
    "   - **Example:** Add a new feature that flags whether the original feature was missing and use this indicator in your model.\n",
    "\n",
    "**In Summary:**\n",
    "Before applying Bernoulli Naive Bayes, handle missing values by imputation, removal, or using indicator variables. The choice of method depends on the amount of missing data and the impact of missing values on your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6d9b7c-f70e-4637-ad22-d59d3f26c0bc",
   "metadata": {},
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification.\n",
    "\n",
    "**How It Works for Multi-Class Classification:**\n",
    "\n",
    "- **Naive Assumption:** Gaussian Naive Bayes assumes that the features follow a Gaussian (normal) distribution within each class.\n",
    "- **Probability Calculation:** For multi-class classification, the algorithm calculates the posterior probability for each class using Bayes' theorem and selects the class with the highest probability as the prediction.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Estimate Parameters:**\n",
    "   - For each class, estimate the mean and variance of the features assuming they follow a Gaussian distribution.\n",
    "\n",
    "2. **Calculate Likelihood:**\n",
    "   - Compute the likelihood of the feature values for each class using the Gaussian probability density function.\n",
    "\n",
    "3. **Apply Bayes' Theorem:**\n",
    "   - Use Bayes' theorem to calculate the posterior probability for each class given the feature values.\n",
    "\n",
    "4. **Class Prediction:**\n",
    "   - Predict the class with the highest posterior probability.\n",
    "\n",
    "**Example:**\n",
    "If you are classifying iris flowers into multiple species based on their features (sepal length, sepal width, petal length, petal width), Gaussian Naive Bayes will compute the probability of each species given the feature values and choose the species with the highest probability.\n",
    "\n",
    "Gaussian Naive Bayes is straightforward and effective for multi-class problems where the feature distributions can be reasonably approximated by Gaussian distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e884af1-c024-490b-8a1d-26be472580a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
