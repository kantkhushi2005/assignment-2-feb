{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f1914e-8a4b-4c58-920d-25001cdebe5e",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "Gradient Boosting Regression is a machine learning technique used for regression tasks. It builds a predictive model by combining the outputs of multiple weak learners, typically decision trees, in a sequential manner. Here’s a summary of how it works:\n",
    "\n",
    "### **Concept:**\n",
    "\n",
    "- **Weak Learners**: Gradient Boosting uses simple models, often shallow decision trees, as weak learners.\n",
    "- **Sequential Training**: Models are trained sequentially, with each new model aiming to correct the errors made by the previous ones.\n",
    "\n",
    "### **How It Works:**\n",
    "\n",
    "1. **Initial Model**: Start with a base model that makes initial predictions. This could be a simple model like a mean prediction for regression.\n",
    "\n",
    "2. **Calculate Residuals**: Compute the residuals, which are the differences between the actual values and the predictions of the base model.\n",
    "\n",
    "3. **Train New Model**: Train a new weak learner to predict the residuals from the previous model. This new model tries to capture the errors that the base model couldn’t address.\n",
    "\n",
    "4. **Update Predictions**: Add the predictions from the new model to the existing predictions. This updates the model to incorporate the new information.\n",
    "\n",
    "5. **Iterate**: Repeat the process of calculating residuals, training new models, and updating predictions for a specified number of iterations or until the model performance stabilizes.\n",
    "\n",
    "6. **Final Prediction**: The final prediction is the sum of the predictions from all models in the ensemble, typically adjusted by a learning rate to control the contribution of each model.\n",
    "\n",
    "### **Key Components:**\n",
    "\n",
    "- **Learning Rate**: Controls the step size in updating predictions. A lower learning rate requires more boosting stages to converge but can improve generalization.\n",
    "- **Loss Function**: Measures the difference between actual and predicted values. Common loss functions for regression include mean squared error (MSE) and mean absolute error (MAE).\n",
    "\n",
    "Gradient Boosting Regression is powerful for capturing complex patterns in data and often provides high accuracy, but it can be computationally intensive and sensitive to hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778c4b4e-85fa-4ae7-8936-e08e1d577416",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared.\n",
    "\n",
    "Here's a simple implementation of Gradient Boosting Regression from scratch using Python and NumPy. We’ll use a synthetic dataset for this example:\n",
    "\n",
    "### **1. Import Libraries**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "```\n",
    "\n",
    "### **2. Define Gradient Boosting Regression**\n",
    "\n",
    "```python\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Initialize model with mean prediction\n",
    "        self.initial_prediction = np.mean(y)\n",
    "        predictions = np.full(y.shape, self.initial_prediction)\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = y - predictions\n",
    "            model = DecisionTreeRegressor(max_depth=3)\n",
    "            model.fit(X, residuals)\n",
    "            self.models.append(model)\n",
    "            update = model.predict(X)\n",
    "            predictions += self.learning_rate * update\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.full(X.shape[0], self.initial_prediction)\n",
    "        for model in self.models:\n",
    "            predictions += self.learning_rate * model.predict(X)\n",
    "        return predictions\n",
    "```\n",
    "\n",
    "### **3. Generate Sample Data**\n",
    "\n",
    "```python\n",
    "# Generate sample data\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 3 * X.squeeze() + np.random.randn(100) * 2\n",
    "```\n",
    "\n",
    "### **4. Train and Evaluate the Model**\n",
    "\n",
    "```python\n",
    "# Initialize and train the model\n",
    "gbr = GradientBoostingRegressor(n_estimators=50, learning_rate=0.1)\n",
    "gbr.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gbr.predict(X)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R-squared: {r2:.2f}\")\n",
    "```\n",
    "\n",
    "### **Explanation:**\n",
    "\n",
    "- **GradientBoostingRegressor Class**: Implements a simple gradient boosting algorithm using decision trees as weak learners.\n",
    "- **fit Method**: Initializes predictions with the mean target value and iteratively fits decision trees to the residuals, updating predictions.\n",
    "- **predict Method**: Combines predictions from all decision trees to make final predictions.\n",
    "- **Evaluation**: Uses Mean Squared Error (MSE) and R-squared to assess the model's performance.\n",
    "\n",
    "This example provides a basic understanding of gradient boosting. For real-world applications, libraries like `scikit-learn` offer optimized and more flexible implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2acf16-bf61-48eb-a2b5-ed16f917a283",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters\n",
    "\n",
    "To optimize the performance of the Gradient Boosting Regressor by experimenting with different hyperparameters, you can use grid search. Here's a concise example using the `GridSearchCV` from `scikit-learn` to find the best hyperparameters for learning rate, number of trees, and tree depth:\n",
    "\n",
    "### **1. Import Libraries**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "```\n",
    "\n",
    "### **2. Generate Sample Data**\n",
    "\n",
    "```python\n",
    "# Generate sample data\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 3 * X.squeeze() + np.random.randn(100) * 2\n",
    "```\n",
    "\n",
    "### **3. Define the Model and Parameter Grid**\n",
    "\n",
    "```python\n",
    "# Initialize the Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "```\n",
    "\n",
    "### **4. Perform Grid Search**\n",
    "\n",
    "```python\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=gbr, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit grid search\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = -grid_search.best_score_  # Negative because scoring was neg_mean_squared_error\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Mean Squared Error: {best_score:.2f}\")\n",
    "```\n",
    "\n",
    "### **Explanation:**\n",
    "\n",
    "1. **Parameter Grid**: Specifies the values to test for `n_estimators`, `learning_rate`, and `max_depth`.\n",
    "2. **GridSearchCV**: Performs an exhaustive search over the parameter grid with cross-validation.\n",
    "3. **Results**: Prints the best combination of hyperparameters and the corresponding mean squared error.\n",
    "\n",
    "This approach systematically explores the parameter space to find the best hyperparameters for your Gradient Boosting Regressor model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5dede3-80fb-4c81-a527-9a2fe2d1cf87",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "To optimize the performance of the Gradient Boosting Regressor by experimenting with different hyperparameters, you can use grid search. Here's a concise example using the `GridSearchCV` from `scikit-learn` to find the best hyperparameters for learning rate, number of trees, and tree depth:\n",
    "\n",
    "### **1. Import Libraries**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "```\n",
    "\n",
    "### **2. Generate Sample Data**\n",
    "\n",
    "```python\n",
    "# Generate sample data\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 3 * X.squeeze() + np.random.randn(100) * 2\n",
    "```\n",
    "\n",
    "### **3. Define the Model and Parameter Grid**\n",
    "\n",
    "```python\n",
    "# Initialize the Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "```\n",
    "\n",
    "### **4. Perform Grid Search**\n",
    "\n",
    "```python\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=gbr, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit grid search\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = -grid_search.best_score_  # Negative because scoring was neg_mean_squared_error\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Mean Squared Error: {best_score:.2f}\")\n",
    "```\n",
    "\n",
    "### **Explanation:**\n",
    "\n",
    "1. **Parameter Grid**: Specifies the values to test for `n_estimators`, `learning_rate`, and `max_depth`.\n",
    "2. **GridSearchCV**: Performs an exhaustive search over the parameter grid with cross-validation.\n",
    "3. **Results**: Prints the best combination of hyperparameters and the corresponding mean squared error.\n",
    "\n",
    "This approach systematically explores the parameter space to find the best hyperparameters for your Gradient Boosting Regressor model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9d26b9-6cad-4384-9be4-2767adc41202",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "The intuition behind Gradient Boosting is to build a strong predictive model by sequentially improving the performance of weak learners. It works as follows:\n",
    "\n",
    "1. **Start Simple**: Begin with a simple model that makes initial predictions.\n",
    "2. **Focus on Errors**: Identify and focus on the errors made by the current model.\n",
    "3. **Correct Errors**: Train a new model specifically to correct these errors.\n",
    "4. **Combine Models**: Add the new model's predictions to the existing ones, refining the overall model.\n",
    "5. **Iterate**: Repeat the process, iteratively correcting errors and combining models to improve prediction accuracy.\n",
    "\n",
    "The idea is to gradually reduce the residual errors by learning from the mistakes of previous models, leading to a more accurate and robust final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61658193-0159-4b6b-afe1-14594672f848",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "Gradient Boosting builds an ensemble of weak learners by:\n",
    "\n",
    "1. **Training Sequentially**: It trains each weak learner (e.g., decision trees) one after another, where each new learner focuses on correcting the errors (residuals) made by the previous learners.\n",
    "2. **Updating Predictions**: Each weak learner's predictions are added to the ensemble's predictions, with the contribution scaled by a learning rate.\n",
    "3. **Combining Models**: The final model is the sum of the predictions from all weak learners, which collectively improve accuracy and reduce errors.\n",
    "\n",
    "This process creates a strong predictive model by incrementally refining the predictions based on the errors of prior models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d61bfa-8161-4f30-b883-e700c59dc676",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?\n",
    "\n",
    "The mathematical intuition behind Gradient Boosting involves these key steps:\n",
    "\n",
    "1. **Initial Prediction**: Start with a simple model (e.g., constant prediction) to initialize predictions.\n",
    "\n",
    "2. **Compute Residuals**: Calculate the residuals (errors) between the actual values and the current predictions.\n",
    "\n",
    "3. **Fit a Weak Learner**: Train a weak learner to predict these residuals, aiming to capture the errors.\n",
    "\n",
    "4. **Update Predictions**: Add the weak learner's predictions to the current model's predictions, scaled by a learning rate.\n",
    "\n",
    "5. **Iterate**: Repeat the process of calculating residuals, training a new weak learner, and updating predictions for a number of iterations.\n",
    "\n",
    "6. **Combine Models**: The final model is a weighted sum of all weak learners' predictions, with each learner contributing to reducing errors.\n",
    "\n",
    "This process refines the model incrementally, improving accuracy by focusing on correcting residual errors from previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05c4974-964a-4eb2-b43f-90f482e53f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
