{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f436b0c6-26e1-432d-ab28-4d8db8022c7b",
   "metadata": {},
   "source": [
    "Q1. You are working on a machine learning project where you have a dataset containing numerical and\n",
    "categorical features. You have identified that some of the features are highly correlated and there are\n",
    "missing values in some of the columns. You want to build a pipeline that automates the feature\n",
    "engineering process and handles the missing values\n",
    "Design a pipeline that includes the following steps\"\n",
    "Use an automated feature selection method to identify the important features in the dataset.\n",
    "Create a numerical pipeline that includes the following steps\"\n",
    "Impute the missing values in the numerical columns using the mean of the column values.\n",
    "Scale the numerical columns using standardisation.\n",
    "Create a categorical pipeline that includes the following steps\"\n",
    "Impute the missing values in the categorical columns using the most frequent value of the column.\n",
    "One-hot encode the categorical columns.\n",
    "Combine the numerical and categorical pipelines using a ColumnTransformer.\n",
    "Use a Random Forest Classifier to build the final model.\n",
    "Evaluate the accuracy of the model on the test dataset.\n",
    "Note! Your solution should include code snippets for each step of the pipeline, and a brief explanation of\n",
    "each step. You should also provide an interpretation of the results and suggest possible improvements for\n",
    "the pipeline.\n",
    "\n",
    "\n",
    "To design a machine learning pipeline that handles missing values, automates feature engineering, and builds a Random Forest classifier, we can use the following steps:\n",
    "\n",
    "1. **Automated Feature Selection**: Identify the important features using a feature selection method.\n",
    "2. **Numerical Pipeline**:\n",
    "   - Impute missing values using the mean of the column values.\n",
    "   - Scale the numerical columns using standardization.\n",
    "3. **Categorical Pipeline**:\n",
    "   - Impute missing values using the most frequent value of the column.\n",
    "   - One-hot encode the categorical columns.\n",
    "4. **Combine Pipelines**: Use `ColumnTransformer` to combine the numerical and categorical pipelines.\n",
    "5. **Build and Evaluate the Model**: Use a Random Forest Classifier to build the model and evaluate its accuracy.\n",
    "\n",
    "Here's the code implementation for each step:\n",
    "\n",
    "### Step 1: Automated Feature Selection\n",
    "We will use `SelectKBest` with `f_classif` for feature selection.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Assuming df is your DataFrame and 'target' is your target column\n",
    "X = df.drop(columns='target')\n",
    "y = df['target']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature selection\n",
    "selector = SelectKBest(score_func=f_classif, k='all')  # Adjust 'k' as needed\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "```\n",
    "\n",
    "### Step 2: Numerical Pipeline\n",
    "Impute missing values and scale the numerical columns.\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "```\n",
    "\n",
    "### Step 3: Categorical Pipeline\n",
    "Impute missing values and one-hot encode the categorical columns.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "```\n",
    "\n",
    "### Step 4: Combine Pipelines using ColumnTransformer\n",
    "Combine the numerical and categorical pipelines.\n",
    "\n",
    "```python\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ])\n",
    "```\n",
    "\n",
    "### Step 5: Build and Evaluate the Model\n",
    "Build the Random Forest classifier and evaluate the model.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Combine the preprocessor and the classifier in a pipeline\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "```\n",
    "\n",
    "### Interpretation of Results and Possible Improvements\n",
    "- **Interpretation**: The accuracy score gives an initial indication of the model's performance. Further evaluation metrics such as precision, recall, and F1-score should be calculated to understand the model's effectiveness, especially if the dataset is imbalanced.\n",
    "- **Possible Improvements**:\n",
    "  - **Feature Engineering**: Additional feature engineering steps such as creating interaction terms or polynomial features.\n",
    "  - **Hyperparameter Tuning**: Perform grid search or random search to find the optimal hyperparameters for the Random Forest classifier.\n",
    "  - **Model Selection**: Experiment with different models like Gradient Boosting, XGBoost, or neural networks.\n",
    "  - **Cross-Validation**: Use cross-validation to get a more reliable estimate of the model's performance.\n",
    "  - **Handling Imbalance**: If the dataset is imbalanced, consider techniques like SMOTE or using class weights in the classifier.\n",
    "\n",
    "This pipeline provides a structured approach to handling missing values, feature selection, and building a robust model using Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48394974-8ab2-47a2-a108-76eae1ed5f66",
   "metadata": {},
   "source": [
    "Q2. Build a pipeline that includes a random forest classifier and a logistic regression classifier, and then\n",
    "use a voting classifier to combine their predictions. Train the pipeline on the iris dataset and evaluate its\n",
    "accuracy.\n",
    "\n",
    "\n",
    "To build a pipeline that includes both a Random Forest classifier and a Logistic Regression classifier, and then use a Voting Classifier to combine their predictions, we can follow these steps:\n",
    "\n",
    "1. **Preprocess the data**: For the Iris dataset, we don't need to handle missing values or categorical features, but we'll scale the numerical features.\n",
    "2. **Build individual classifiers**: Random Forest and Logistic Regression.\n",
    "3. **Combine classifiers using a Voting Classifier**.\n",
    "4. **Train the pipeline** on the Iris dataset.\n",
    "5. **Evaluate the accuracy** of the combined model.\n",
    "\n",
    "Here's the implementation in Python using scikit-learn:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline for scaling\n",
    "preprocessing_pipeline = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Preprocess the training and test sets\n",
    "X_train_scaled = preprocessing_pipeline.fit_transform(X_train)\n",
    "X_test_scaled = preprocessing_pipeline.transform(X_test)\n",
    "\n",
    "# Define the individual classifiers\n",
    "random_forest = RandomForestClassifier(random_state=42)\n",
    "logistic_regression = LogisticRegression(random_state=42, max_iter=200)\n",
    "\n",
    "# Combine classifiers using a Voting Classifier\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', random_forest),\n",
    "        ('lr', logistic_regression)\n",
    "    ],\n",
    "    voting='soft'  # 'soft' voting uses predicted probabilities\n",
    ")\n",
    "\n",
    "# Define the final pipeline\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing_pipeline),\n",
    "    ('classifier', voting_classifier)\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "```\n",
    "\n",
    "### Explanation of Each Step\n",
    "\n",
    "1. **Load the Iris dataset**: We use `load_iris()` from scikit-learn to load the dataset.\n",
    "2. **Split the data**: The dataset is split into training and test sets using `train_test_split`.\n",
    "3. **Preprocessing pipeline**: A pipeline is created to scale the numerical features using `StandardScaler`.\n",
    "4. **Individual classifiers**: We define a Random Forest classifier and a Logistic Regression classifier.\n",
    "5. **Voting Classifier**: We use a `VotingClassifier` to combine the predictions of the Random Forest and Logistic Regression classifiers. The `voting='soft'` parameter uses the predicted probabilities to make the final prediction.\n",
    "6. **Final pipeline**: The preprocessing pipeline and the Voting Classifier are combined into a single pipeline.\n",
    "7. **Train the pipeline**: The pipeline is trained on the training data.\n",
    "8. **Make predictions**: The trained pipeline is used to make predictions on the test set.\n",
    "9. **Evaluate accuracy**: The accuracy of the combined model is evaluated using `accuracy_score`.\n",
    "\n",
    "### Interpretation of Results\n",
    "- **Accuracy**: The accuracy score provides a measure of how well the combined model is performing on the test set. In this case, it will be displayed in the output.\n",
    "- **Further Evaluation**: Additional evaluation metrics such as precision, recall, and F1-score can be calculated to gain deeper insights into the model's performance, especially if the dataset has class imbalances.\n",
    "\n",
    "This pipeline demonstrates how to combine multiple classifiers using a Voting Classifier to potentially improve the overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43916b9-66f5-49bc-b6d3-509c83b56b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
