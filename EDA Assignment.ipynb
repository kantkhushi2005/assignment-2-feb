{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d7f13e8-8099-41a0-ab41-f69e90a94987",
   "metadata": {},
   "source": [
    "Q1. What are the key features of the wine quality data set? Discuss the importance of each feature in\n",
    "predicting the quality of wine.\n",
    "\n",
    "The wine quality dataset typically contains several chemical properties of wine, which are used to predict wine quality. The key features usually include:\n",
    "\n",
    "1. **Fixed Acidity:** Primarily tartaric acid. Important as it contributes to the wine's tartness.\n",
    "2. **Volatile Acidity:** Mainly acetic acid. High levels can lead to an unpleasant, vinegar taste.\n",
    "3. **Citric Acid:** Adds freshness and flavor.\n",
    "4. **Residual Sugar:** Sugar remaining after fermentation. Higher levels can make wine taste sweeter.\n",
    "5. **Chlorides:** Amount of salt. High levels can indicate poor quality.\n",
    "6. **Free Sulfur Dioxide:** Protects wine from oxidation and microbial spoilage.\n",
    "7. **Total Sulfur Dioxide:** Sum of free and bound forms. Important for preservation.\n",
    "8. **Density:** Influenced by alcohol and sugar content. Helps in distinguishing different wine types.\n",
    "9. **pH:** Measures acidity. Influences the taste and preservation.\n",
    "10. **Sulphates:** Adds to wine's sulfur dioxide levels and helps in preservation.\n",
    "11. **Alcohol:** Higher levels can lead to better sensory qualities.\n",
    "\n",
    "### Importance of Each Feature in Predicting Wine Quality:\n",
    "- **Acidity (Fixed, Volatile, Citric):** Directly affects taste and mouthfeel. Balanced acidity is crucial for quality.\n",
    "- **Residual Sugar:** Influences sweetness, affecting consumer preference.\n",
    "- **Chlorides:** High levels can negatively affect taste.\n",
    "- **Sulfur Dioxide (Free, Total):** Essential for wine preservation; however, excessive amounts can cause off-flavors.\n",
    "- **Density:** Correlates with sugar and alcohol content, indirectly affecting flavor.\n",
    "- **pH:** Balances taste, microbial stability, and aging potential.\n",
    "- **Sulphates:** Affect preservation and overall stability.\n",
    "- **Alcohol:** Contributes to body, flavor, and overall quality perception.\n",
    "\n",
    "Each feature contributes to the wine's overall profile and quality perception, making them important for predicting the quality score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18311a6-4964-4421-9e90-65d38fa9afdd",
   "metadata": {},
   "source": [
    "Q2. How did you handle missing data in the wine quality data set during the feature engineering process?\n",
    "Discuss the advantages and disadvantages of different imputation techniques.\n",
    "\n",
    "Handling missing data is a crucial step in feature engineering. Here’s a discussion on how one might handle missing data in the wine quality dataset and the advantages and disadvantages of different imputation techniques.\n",
    "\n",
    "### Handling Missing Data:\n",
    "\n",
    "1. **Identifying Missing Data:** First, identify missing values using functions like `isnull()` or `isna()` in pandas.\n",
    "\n",
    "2. **Choosing Imputation Techniques:**\n",
    "   - **Remove Missing Data:** Remove rows or columns with missing values.\n",
    "   - **Impute with Mean/Median/Mode:** Replace missing values with the mean, median, or mode of the column.\n",
    "   - **Impute with Regression:** Use regression models to predict and replace missing values.\n",
    "   - **K-Nearest Neighbors (KNN) Imputation:** Use the mean of the nearest neighbors to impute missing values.\n",
    "   - **Advanced Techniques:** Use algorithms like MICE (Multiple Imputation by Chained Equations) or deep learning-based approaches.\n",
    "\n",
    "### Advantages and Disadvantages of Imputation Techniques:\n",
    "\n",
    "1. **Removing Missing Data:**\n",
    "   - **Advantages:** Simple and straightforward. No additional assumptions are needed.\n",
    "   - **Disadvantages:** Loss of information, which can lead to biased results if a significant portion of data is missing.\n",
    "\n",
    "2. **Impute with Mean/Median/Mode:**\n",
    "   - **Advantages:** Simple to implement. Useful for numerical data (mean/median) and categorical data (mode).\n",
    "   - **Disadvantages:** Does not account for the relationship between features. Can introduce bias and reduce variability in the dataset.\n",
    "\n",
    "3. **Impute with Regression:**\n",
    "   - **Advantages:** Takes into account the relationship between features. Can provide more accurate imputations.\n",
    "   - **Disadvantages:** Computationally more expensive. Assumes linear relationships, which may not always be true.\n",
    "\n",
    "4. **K-Nearest Neighbors (KNN) Imputation:**\n",
    "   - **Advantages:** Considers the relationship between features. Can handle both numerical and categorical data.\n",
    "   - **Disadvantages:** Computationally intensive, especially for large datasets. Requires careful tuning of parameters like the number of neighbors.\n",
    "\n",
    "5. **Advanced Techniques (e.g., MICE, Deep Learning):**\n",
    "   - **Advantages:** Can handle complex relationships and interactions between features. Often provides the most accurate imputations.\n",
    "   - **Disadvantages:** Very computationally intensive. Requires more complex implementation and understanding.\n",
    "\n",
    "### Example Process:\n",
    "\n",
    "For the wine quality dataset, a typical approach might be:\n",
    "\n",
    "1. **Remove Missing Data:** If only a small number of entries are missing.\n",
    "2. **Mean/Median Imputation:** For features like `fixed acidity`, `volatile acidity`, etc., if missing values are randomly distributed.\n",
    "3. **KNN Imputation:** If the dataset has a substantial amount of missing data and features have strong correlations.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "The choice of imputation technique depends on the nature of the missing data, the dataset size, and the relationships between features. Simpler techniques like mean/median imputation are often sufficient, but more sophisticated methods like KNN or regression imputation can yield better results, especially when dealing with larger datasets and more complex missing data patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272c3f2f-04a0-457d-a4d6-169458bc725e",
   "metadata": {},
   "source": [
    "Q3. What are the key factors that affect students' performance in exams? How would you go about\n",
    "analyzing these factors using statistical techniques?\n",
    "\n",
    "Several key factors can affect students' performance in exams. These factors can be broadly categorized into individual characteristics, family background, school environment, and socio-economic status. Here’s a detailed look at these factors and how to analyze them using statistical techniques.\n",
    "\n",
    "### Key Factors Affecting Students' Performance:\n",
    "\n",
    "1. **Individual Characteristics:**\n",
    "   - **Study Habits:** Time spent studying, quality of study sessions.\n",
    "   - **Attendance:** Regularity and punctuality in attending classes.\n",
    "   - **Health:** Physical and mental health status.\n",
    "   - **Motivation:** Level of intrinsic and extrinsic motivation.\n",
    "\n",
    "2. **Family Background:**\n",
    "   - **Parental Education:** Level of parents' education.\n",
    "   - **Parental Involvement:** Extent of parental support and involvement in education.\n",
    "   - **Home Environment:** Availability of study materials and a conducive study environment.\n",
    "\n",
    "3. **School Environment:**\n",
    "   - **Teacher Quality:** Experience and teaching methods of teachers.\n",
    "   - **Class Size:** Number of students per class.\n",
    "   - **School Facilities:** Availability of resources like libraries, laboratories, etc.\n",
    "\n",
    "4. **Socio-Economic Status:**\n",
    "   - **Income Level:** Family income and its impact on educational resources.\n",
    "   - **Neighborhood:** Quality of the neighborhood and peer influences.\n",
    "\n",
    "### Analyzing Factors Using Statistical Techniques:\n",
    "\n",
    "1. **Data Collection:**\n",
    "   - Gather data on the above factors through surveys, school records, and standardized tests.\n",
    "   - Ensure data on students' exam performance is also collected.\n",
    "\n",
    "2. **Exploratory Data Analysis (EDA):**\n",
    "   - **Descriptive Statistics:** Summarize the data using mean, median, mode, standard deviation, etc.\n",
    "   - **Visualization:** Use histograms, box plots, and scatter plots to visualize the distribution and relationships between variables.\n",
    "\n",
    "3. **Correlation Analysis:**\n",
    "   - **Pearson/Spearman Correlation:** Assess the strength and direction of the relationship between continuous variables (e.g., study hours and exam scores).\n",
    "\n",
    "4. **Regression Analysis:**\n",
    "   - **Linear Regression:** Model the relationship between exam performance (dependent variable) and various independent variables (e.g., study hours, parental education).\n",
    "   - **Multiple Regression:** Include multiple predictors to understand their combined effect on exam performance.\n",
    "   - **Logistic Regression:** If the performance is categorized (e.g., pass/fail), logistic regression can be used.\n",
    "\n",
    "5. **Analysis of Variance (ANOVA):**\n",
    "   - **One-way ANOVA:** Compare the means of exam scores across different groups (e.g., different levels of parental education).\n",
    "   - **Two-way ANOVA:** Examine the interaction effect between two categorical factors (e.g., parental involvement and school facilities).\n",
    "\n",
    "6. **Multivariate Analysis:**\n",
    "   - **Principal Component Analysis (PCA):** Reduce the dimensionality of data and identify key factors.\n",
    "   - **Cluster Analysis:** Group students based on similar characteristics and performance.\n",
    "\n",
    "7. **Machine Learning Techniques:**\n",
    "   - **Decision Trees:** Identify the most significant factors affecting performance and visualize decision rules.\n",
    "   - **Random Forests:** Use an ensemble of decision trees to improve predictive accuracy.\n",
    "   - **Support Vector Machines (SVM):** Classify students based on performance predictors.\n",
    "\n",
    "### Example Analysis Workflow:\n",
    "\n",
    "1. **Data Cleaning:** Handle missing values, outliers, and ensure data consistency.\n",
    "2. **EDA:** Summarize and visualize the data to get initial insights.\n",
    "3. **Correlation Matrix:** Calculate correlations between exam scores and other variables.\n",
    "4. **Regression Models:** Fit linear and multiple regression models to predict exam performance.\n",
    "5. **ANOVA:** Conduct ANOVA tests to compare means across different groups.\n",
    "6. **PCA/Cluster Analysis:** Reduce dimensionality and identify patterns.\n",
    "7. **Machine Learning Models:** Build and evaluate predictive models to identify key predictors.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "By systematically collecting data and applying statistical techniques, you can identify and analyze the key factors affecting students' performance in exams. This analysis can help in developing targeted interventions to improve educational outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf2ef12-8ac6-4e96-b037-7d262110606f",
   "metadata": {},
   "source": [
    "Q4. Describe the process of feature engineering in the context of the student performance data set. How\n",
    "did you select and transform the variables for your model?\n",
    "\n",
    "Feature engineering is a crucial step in preparing data for machine learning models, as it involves selecting, transforming, and creating features that improve the performance of the model. Here's a detailed process of feature engineering in the context of a student performance dataset:\n",
    "\n",
    "### Process of Feature Engineering\n",
    "\n",
    "1. **Understanding the Data:**\n",
    "   - Identify the target variable (e.g., exam scores or pass/fail status).\n",
    "   - Explore the available features, such as demographic information, academic history, attendance, study habits, family background, etc.\n",
    "\n",
    "2. **Data Cleaning:**\n",
    "   - **Handle Missing Values:** Impute missing values using techniques like mean/median imputation or more advanced methods like KNN imputation.\n",
    "   - **Remove Outliers:** Detect and handle outliers using statistical methods (e.g., z-scores) or domain knowledge.\n",
    "   - **Correct Errors:** Fix any obvious data entry errors.\n",
    "\n",
    "3. **Feature Selection:**\n",
    "   - **Correlation Analysis:** Use Pearson or Spearman correlation to identify features that have a strong relationship with the target variable.\n",
    "   - **Variance Threshold:** Remove features with low variance, as they provide little information.\n",
    "   - **Domain Knowledge:** Include features that are known to be important based on educational research or domain expertise.\n",
    "\n",
    "4. **Feature Transformation:**\n",
    "   - **Normalization/Standardization:** Scale numerical features to have a mean of 0 and a standard deviation of 1.\n",
    "   - **Log Transformation:** Apply log transformation to skewed features to make them more normally distributed.\n",
    "   - **Categorical Encoding:** Convert categorical features to numerical format using techniques like one-hot encoding, label encoding, or ordinal encoding.\n",
    "\n",
    "5. **Feature Creation:**\n",
    "   - **Interaction Features:** Create new features by combining existing ones (e.g., interaction terms between study hours and parental involvement).\n",
    "   - **Polynomial Features:** Generate polynomial features to capture non-linear relationships.\n",
    "   - **Aggregated Features:** Create aggregated features like mean, sum, or count for different groups (e.g., average study hours per week).\n",
    "\n",
    "6. **Dimensionality Reduction:**\n",
    "   - **Principal Component Analysis (PCA):** Reduce the dimensionality of the dataset while retaining most of the variance.\n",
    "   - **Feature Selection Methods:** Use techniques like Recursive Feature Elimination (RFE) or feature importance from models like Random Forest to select the most relevant features.\n",
    "\n",
    "### Example of Feature Engineering Steps:\n",
    "\n",
    "1. **Initial Dataset:**\n",
    "   - Features: `age`, `gender`, `study_hours`, `attendance`, `parental_education`, `free_lunch`, `extracurricular_activities`, `exam_scores`\n",
    "\n",
    "2. **Data Cleaning:**\n",
    "   - Handle missing values in `parental_education` using median imputation.\n",
    "   - Remove outliers in `study_hours`.\n",
    "\n",
    "3. **Feature Selection:**\n",
    "   - Perform correlation analysis to identify features strongly correlated with `exam_scores`.\n",
    "   - Use domain knowledge to retain `study_hours`, `attendance`, `parental_education`, and `free_lunch`.\n",
    "\n",
    "4. **Feature Transformation:**\n",
    "   - Normalize `study_hours` and `attendance`.\n",
    "   - Apply one-hot encoding to `gender` and `free_lunch`.\n",
    "\n",
    "5. **Feature Creation:**\n",
    "   - Create an interaction feature between `study_hours` and `parental_education`.\n",
    "   - Generate polynomial features for `study_hours`.\n",
    "\n",
    "6. **Dimensionality Reduction:**\n",
    "   - Apply PCA to reduce dimensionality while retaining most variance.\n",
    "\n",
    "### Transformed Variables for Model:\n",
    "\n",
    "- **Numerical Features:** `study_hours_normalized`, `attendance_normalized`\n",
    "- **Categorical Features:** `gender_encoded`, `free_lunch_encoded`\n",
    "- **Created Features:** `study_hours_parental_education_interaction`, `study_hours_squared`\n",
    "- **Reduced Features:** Principal components from PCA\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "Feature engineering involves a combination of selecting relevant features, transforming existing features, and creating new features to improve the performance of a machine learning model. By carefully engineering features, we can enhance the model's ability to predict student performance accurately. This process requires a mix of statistical techniques and domain knowledge to ensure the features are meaningful and informative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955fe0db-29e7-446a-ba79-69529700d7d6",
   "metadata": {},
   "source": [
    "Q5. Load the wine quality data set and perform exploratory data analysis (EDA) to identify the distribution\n",
    "of each feature. Which feature(s) exhibit non-normality, and what transformations could be applied to\n",
    "these features to improve normality?\n",
    "\n",
    "To perform Exploratory Data Analysis (EDA) on the wine quality dataset and identify the distribution of each feature, you would follow these steps:\n",
    "\n",
    "### 1. **Load the Wine Quality Dataset**\n",
    "\n",
    "Assuming you have the dataset in CSV format, you can load it using Python with pandas:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('winequality-red.csv')  # Replace with the path to your dataset\n",
    "```\n",
    "\n",
    "### 2. **Initial Inspection**\n",
    "\n",
    "Get a sense of the data using basic commands:\n",
    "\n",
    "```python\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Summary statistics of the dataset\n",
    "print(data.describe())\n",
    "\n",
    "# Information about the dataset\n",
    "print(data.info())\n",
    "```\n",
    "\n",
    "### 3. **Distribution of Each Feature**\n",
    "\n",
    "You can use visualization tools to examine the distribution of each feature. Histograms and box plots are commonly used for this purpose.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Plot histograms for each feature\n",
    "for i, column in enumerate(data.columns):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    sns.histplot(data[column], kde=True)\n",
    "    plt.title(column)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 4. **Check for Normality**\n",
    "\n",
    "To formally assess normality, you can use statistical tests and plots:\n",
    "\n",
    "- **Shapiro-Wilk Test:** Tests the null hypothesis that the data was drawn from a normal distribution.\n",
    "- **Q-Q Plot:** Plots quantiles of the feature against quantiles of a normal distribution.\n",
    "\n",
    "```python\n",
    "from scipy.stats import shapiro, normaltest\n",
    "\n",
    "# Shapiro-Wilk Test for normality\n",
    "for column in data.columns:\n",
    "    stat, p_value = shapiro(data[column].dropna())\n",
    "    print(f'{column}: Shapiro-Wilk p-value = {p_value}')\n",
    "\n",
    "# Alternatively, use D'Agostino's K-squared test\n",
    "for column in data.columns:\n",
    "    stat, p_value = normaltest(data[column].dropna())\n",
    "    print(f'{column}: D\\'Agostino\\'s K-squared p-value = {p_value}')\n",
    "```\n",
    "\n",
    "### 5. **Transformations for Non-Normal Features**\n",
    "\n",
    "Based on the results from the normality tests and visualizations, you can apply transformations to features that exhibit non-normality:\n",
    "\n",
    "- **Log Transformation:** Useful for skewed distributions.\n",
    "    ```python\n",
    "    data['feature_log'] = np.log1p(data['feature'])\n",
    "    ```\n",
    "\n",
    "- **Square Root Transformation:** Also helps with reducing skew.\n",
    "    ```python\n",
    "    data['feature_sqrt'] = np.sqrt(data['feature'])\n",
    "    ```\n",
    "\n",
    "- **Box-Cox Transformation:** A family of transformations that includes log and power transformations. Requires positive values.\n",
    "    ```python\n",
    "    from scipy.stats import boxcox\n",
    "\n",
    "    data['feature_boxcox'], _ = boxcox(data['feature'] + 1)  # Add 1 if there are zero values\n",
    "    ```\n",
    "\n",
    "- **Yeo-Johnson Transformation:** Handles both positive and negative values.\n",
    "    ```python\n",
    "    from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "    pt = PowerTransformer(method='yeo-johnson')\n",
    "    data['feature_yeojohnson'] = pt.fit_transform(data[['feature']])\n",
    "    ```\n",
    "\n",
    "### Example Analysis\n",
    "\n",
    "Assuming we find that features such as `fixed acidity`, `volatile acidity`, and `residual sugar` exhibit non-normality based on the histogram and normality tests:\n",
    "\n",
    "1. **Fixed Acidity:**\n",
    "   - **Non-Normality:** Positively skewed.\n",
    "   - **Transformation:** Apply log transformation.\n",
    "\n",
    "2. **Volatile Acidity:**\n",
    "   - **Non-Normality:** Positively skewed.\n",
    "   - **Transformation:** Apply log transformation or Box-Cox transformation.\n",
    "\n",
    "3. **Residual Sugar:**\n",
    "   - **Non-Normality:** Highly skewed.\n",
    "   - **Transformation:** Apply log or square root transformation.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "By performing EDA and applying appropriate transformations, you can address non-normality in your dataset, which can improve the performance of statistical models and machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee4173d-fc60-4701-9f8a-21921fc69216",
   "metadata": {},
   "source": [
    "Q6. Using the wine quality data set, perform principal component analysis (PCA) to reduce the number of\n",
    "features. What is the minimum number of principal components required to explain 90% of the variance in\n",
    "the data?\n",
    "\n",
    "To perform Principal Component Analysis (PCA) on the wine quality dataset and determine the minimum number of principal components required to explain 90% of the variance, follow these steps:\n",
    "\n",
    "### 1. **Load the Dataset**\n",
    "\n",
    "First, load the dataset and prepare it for PCA:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('winequality-red.csv')  # Replace with the path to your dataset\n",
    "\n",
    "# Drop the target variable if present, assume 'quality' is the target\n",
    "X = data.drop('quality', axis=1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "### 2. **Perform PCA**\n",
    "\n",
    "Fit the PCA model and compute the explained variance:\n",
    "\n",
    "```python\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# Compute explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = explained_variance_ratio.cumsum()\n",
    "```\n",
    "\n",
    "### 3. **Determine the Minimum Number of Principal Components**\n",
    "\n",
    "Find the number of principal components required to explain at least 90% of the variance:\n",
    "\n",
    "```python\n",
    "# Plot the cumulative explained variance\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(cumulative_explained_variance, marker='o')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance vs. Number of Principal Components')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Find the number of components that explain at least 90% of the variance\n",
    "num_components_90 = next(i for i, v in enumerate(cumulative_explained_variance) if v >= 0.90) + 1\n",
    "print(f'Minimum number of principal components required to explain 90% of the variance: {num_components_90}')\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "1. **Standardization:** PCA requires standardization of features so that each feature contributes equally. This is achieved by scaling each feature to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "2. **Fit PCA:** PCA is fitted to the standardized data, and the explained variance ratio is computed for each principal component.\n",
    "\n",
    "3. **Cumulative Explained Variance:** The cumulative sum of the explained variance ratios is plotted to visualize how the variance is explained as more components are added.\n",
    "\n",
    "4. **Determine Components:** The minimum number of principal components required to explain 90% of the variance is found by locating where the cumulative explained variance exceeds 90%.\n",
    "\n",
    "### Example Output\n",
    "\n",
    "After performing these steps, you will find the minimum number of principal components required to explain 90% of the variance. For example, if the cumulative variance plot shows that 10 components are needed to reach or exceed 90%, then `num_components_90` will be 10.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "PCA helps reduce the dimensionality of the dataset while retaining most of the variance, making it easier to visualize and analyze the data. By determining the minimum number of principal components required to explain a significant portion of the variance (e.g., 90%), you can efficiently reduce the number of features in your dataset while preserving the essential information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38153452-546a-4cb2-8a50-e561ef59ab21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
