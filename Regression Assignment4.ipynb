{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "705e7375-3227-4fae-a835-67f9e6641de8",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "\n",
    "**Lasso Regression**:\n",
    "\n",
    "- **Definition**: Lasso (Least Absolute Shrinkage and Selection Operator) regression is a type of regularized linear regression that adds a penalty proportional to the absolute values of the coefficients, known as L1 regularization. This encourages sparsity by shrinking some coefficients to exactly zero, effectively performing feature selection.\n",
    "\n",
    "- **Equation**:\n",
    "  \\[ \\text{Loss} = \\text{MSE} + \\lambda \\sum_{i=1}^n |\\beta_i| \\]\n",
    "  where \\( \\lambda \\) is the regularization parameter.\n",
    "\n",
    "**Differences from Other Regression Techniques**:\n",
    "\n",
    "1. **Feature Selection**:\n",
    "   - **Lasso Regression**: Performs automatic feature selection by setting some coefficients to zero.\n",
    "   - **Ridge Regression**: Shrinks coefficients but does not set them to zero, so all features remain in the model.\n",
    "\n",
    "2. **Penalty Type**:\n",
    "   - **Lasso Regression**: Uses L1 norm (absolute value) for regularization, promoting sparsity.\n",
    "   - **Ridge Regression**: Uses L2 norm (squared value) for regularization, leading to coefficient shrinkage but not zeroing out.\n",
    "\n",
    "3. **Interpretability**:\n",
    "   - **Lasso Regression**: Can lead to simpler models with fewer predictors, enhancing interpretability.\n",
    "   - **Ridge Regression**: Retains all predictors, which can make the model less interpretable if many predictors are included.\n",
    "\n",
    "**Summary**:\n",
    "Lasso Regression differs by performing feature selection through L1 regularization, making it useful for creating simpler models with fewer predictors compared to techniques like Ridge Regression that only shrink coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35968eff-29a9-4dd3-a8ef-98752eea4e67",
   "metadata": {},
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "**Main Advantage of Lasso Regression in Feature Selection**:\n",
    "\n",
    "- **Automatic Feature Selection**: Lasso Regression can shrink some coefficients to exactly zero, effectively removing irrelevant features from the model. This results in a simpler, more interpretable model by retaining only the most important predictors.\n",
    "\n",
    "**Summary**:\n",
    "The primary advantage of Lasso Regression is its ability to perform automatic feature selection, simplifying the model by eliminating less important predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defbba37-148d-498e-a61e-85440cd2220d",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "\n",
    "**Interpreting Coefficients of Lasso Regression**:\n",
    "\n",
    "- **Non-Zero Coefficients**: Coefficients that are non-zero indicate predictors that have a significant impact on the dependent variable. The magnitude of these coefficients reflects the strength of their relationship with the outcome.\n",
    "\n",
    "- **Zero Coefficients**: Coefficients set to zero by Lasso indicate predictors that are not contributing to the model and have been excluded through feature selection.\n",
    "\n",
    "**Summary**:\n",
    "In Lasso Regression, non-zero coefficients highlight important predictors, while zero coefficients signify excluded features, simplifying the model and aiding interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29baa3c9-73f4-452d-8ce6-3149e7bba15f",
   "metadata": {},
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "model's performance?\n",
    "\n",
    "**Tuning Parameters in Lasso Regression**:\n",
    "\n",
    "1. **Regularization Parameter (λ)**:\n",
    "   - **Definition**: Controls the strength of the penalty applied to the coefficients. \n",
    "   - **Effect on Performance**:\n",
    "     - **Higher λ**: Increases regularization, leading to more coefficients being shrunk to zero and a simpler model. However, excessive regularization can lead to underfitting.\n",
    "     - **Lower λ**: Reduces regularization, allowing more predictors to remain in the model and potentially leading to overfitting if too little regularization is applied.\n",
    "\n",
    "**Summary**:\n",
    "The key tuning parameter in Lasso Regression is λ. Adjusting λ affects the balance between model complexity and fit, influencing feature selection and overall model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd54a7e8-2448-4b61-8074-ea10a1024f6c",
   "metadata": {},
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "\n",
    "**Lasso Regression and Non-Linear Problems**:\n",
    "\n",
    "- **Direct Use**: Lasso Regression itself is designed for linear regression and does not directly handle non-linear relationships.\n",
    "\n",
    "- **How to Use**: To apply Lasso to non-linear problems:\n",
    "  - **Feature Engineering**: Transform the features to capture non-linear relationships (e.g., using polynomial features or interaction terms).\n",
    "  - **Non-Linear Models**: Combine Lasso with non-linear models or algorithms, such as Lasso with kernel methods or other non-linear techniques.\n",
    "\n",
    "**Summary**:\n",
    "Lasso Regression can be adapted for non-linear problems by transforming features to capture non-linearity or by combining it with non-linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462f16f3-41c4-4283-b028-60c4d3b56c79",
   "metadata": {},
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "\n",
    "**Differences Between Ridge Regression and Lasso Regression**:\n",
    "\n",
    "1. **Penalty Type**:\n",
    "   - **Ridge Regression**: Uses L2 regularization (squared coefficients), which shrinks coefficients but does not set them to zero.\n",
    "   - **Lasso Regression**: Uses L1 regularization (absolute coefficients), which can shrink some coefficients to zero, performing feature selection.\n",
    "\n",
    "2. **Feature Selection**:\n",
    "   - **Ridge Regression**: Does not perform feature selection; all predictors remain in the model.\n",
    "   - **Lasso Regression**: Performs feature selection by setting some coefficients to zero, effectively excluding some features.\n",
    "\n",
    "3. **Model Complexity**:\n",
    "   - **Ridge Regression**: Reduces model complexity by shrinking coefficients but retains all features.\n",
    "   - **Lasso Regression**: Reduces model complexity by shrinking and eliminating some coefficients, resulting in a simpler model with fewer predictors.\n",
    "\n",
    "**Summary**:\n",
    "Ridge Regression shrinks coefficients but keeps all features, while Lasso Regression performs feature selection by shrinking some coefficients to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf22f7a5-807e-4901-92db-772a8e1fff24",
   "metadata": {},
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "\n",
    "**Lasso Regression and Multicollinearity**:\n",
    "\n",
    "- **Handling Multicollinearity**: Lasso Regression can help manage multicollinearity by shrinking some of the coefficients of correlated predictors to zero, effectively selecting a subset of features and reducing redundancy.\n",
    "\n",
    "- **How It Works**: By applying L1 regularization, Lasso not only shrinks coefficients but also eliminates some, which reduces the impact of multicollinear variables and simplifies the model.\n",
    "\n",
    "**Summary**:\n",
    "Lasso Regression can address multicollinearity by shrinking and setting some coefficients to zero, thus reducing redundancy and simplifying the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49d299e-0d32-447b-bedf-944fe2d34593",
   "metadata": {},
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "\n",
    "**Choosing the Optimal λ in Lasso Regression**:\n",
    "\n",
    "1. **Cross-Validation**: Use k-fold cross-validation to evaluate different λ values. Select the λ that minimizes the cross-validated error, typically Mean Squared Error (MSE).\n",
    "\n",
    "2. **Grid Search**: Systematically search through a range of λ values and choose the one with the best cross-validation performance.\n",
    "\n",
    "3. **Regularization Path Algorithms**: Use algorithms like LARS (Least Angle Regression) to compute solutions for a range of λ values efficiently.\n",
    "\n",
    "**Summary**:\n",
    "Optimal λ is chosen by using cross-validation or grid search to minimize prediction error, with algorithms like LARS offering efficient computation for multiple λ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39caf33-f67b-4d52-a894-8cc015e06401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
