{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd6cca26-cf6d-41de-9306-4c7e08072b93",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "\n",
    "The K-Nearest Neighbors (KNN) algorithm is a simple, non-parametric, and lazy learning algorithm used for classification and regression tasks. Here's a brief overview:\n",
    "\n",
    "1. **Instance-Based Learning**: KNN is an instance-based learning algorithm, meaning it does not explicitly learn a model but makes predictions based on the entire dataset.\n",
    "\n",
    "2. **How It Works**:\n",
    "   - **Classification**: To classify a new data point, KNN identifies the 'k' nearest data points in the training set and assigns the most common class among these neighbors.\n",
    "   - **Regression**: For regression, KNN predicts the value based on the average (or weighted average) of the 'k' nearest neighbors' values.\n",
    "\n",
    "3. **Distance Metric**: The algorithm uses a distance metric (commonly Euclidean distance) to find the nearest neighbors.\n",
    "\n",
    "4. **Hyperparameters**:\n",
    "   - **'k' (number of neighbors)**: The number of nearest neighbors to consider. Choosing the right 'k' is crucial for the algorithm's performance.\n",
    "   - **Distance Metric**: The method used to calculate the distance between points, such as Euclidean, Manhattan, or Minkowski distance.\n",
    "\n",
    "5. **Advantages**:\n",
    "   - Simple to implement and understand.\n",
    "   - No training phase, making it fast for small datasets.\n",
    "\n",
    "6. **Disadvantages**:\n",
    "   - Computationally expensive for large datasets, as it requires distance calculations for all training points.\n",
    "   - Performance can degrade with irrelevant or redundant features.\n",
    "\n",
    "KNN is effective for small to medium-sized datasets and provides a simple and intuitive approach to both classification and regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c825304-f1ee-450a-a79c-c0c7a7f7f4d2",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "Choosing the value of \\( k \\) in K-Nearest Neighbors (KNN) is crucial for the algorithm's performance. Here are some common methods to choose the optimal \\( k \\):\n",
    "\n",
    "1. **Cross-Validation**: Use cross-validation techniques (e.g., k-fold cross-validation) to test different values of \\( k \\) and select the one that provides the best performance on the validation set.\n",
    "\n",
    "2. **Elbow Method**: Plot the error rate (or accuracy) for different values of \\( k \\). Look for an \"elbow point\" where the error rate starts to level off. This point often represents a good balance between bias and variance.\n",
    "\n",
    "3. **Odd Values for Binary Classification**: Use odd values for \\( k \\) to avoid ties when classifying new instances in binary classification tasks.\n",
    "\n",
    "4. **Domain Knowledge**: Sometimes domain-specific knowledge can guide the choice of \\( k \\).\n",
    "\n",
    "5. **Experimentation**: Start with a small value of \\( k \\) (like 1 or 3) and gradually increase it, observing the impact on performance. Smaller values of \\( k \\) can lead to high variance (overfitting), while larger values can lead to high bias (underfitting).\n",
    "\n",
    "Ultimately, the optimal \\( k \\) is problem-specific and should be determined through careful experimentation and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ff1323-0b0d-4493-aced-c0323090a364",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "The K-Nearest Neighbors (KNN) algorithm can be used for both classification and regression tasks. Here are the key differences between the KNN classifier and the KNN regressor:\n",
    "\n",
    "### KNN Classifier\n",
    "\n",
    "1. **Purpose**: The KNN classifier is used for classification tasks where the goal is to assign a class label to a new data point based on the majority class of its nearest neighbors.\n",
    "2. **Output**: The output is a discrete class label.\n",
    "3. **How It Works**:\n",
    "   - Identify the 'k' nearest neighbors of the new data point.\n",
    "   - Count the frequency of each class among these neighbors.\n",
    "   - Assign the class with the highest frequency to the new data point.\n",
    "4. **Distance Metric**: Commonly uses Euclidean distance, but other distance metrics can also be used.\n",
    "5. **Example Use Case**: Classifying emails as spam or not spam.\n",
    "\n",
    "### KNN Regressor\n",
    "\n",
    "1. **Purpose**: The KNN regressor is used for regression tasks where the goal is to predict a continuous value for a new data point based on the values of its nearest neighbors.\n",
    "2. **Output**: The output is a continuous value.\n",
    "3. **How It Works**:\n",
    "   - Identify the 'k' nearest neighbors of the new data point.\n",
    "   - Calculate the average (or weighted average) of the target values of these neighbors.\n",
    "   - Assign this average value to the new data point.\n",
    "4. **Distance Metric**: Similar to the classifier, it typically uses Euclidean distance but can use other metrics as well.\n",
    "5. **Example Use Case**: Predicting house prices based on features like size, location, and number of bedrooms.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **KNN Classifier**: Used for categorical output (class labels). It assigns the most common class among the nearest neighbors to the new data point.\n",
    "- **KNN Regressor**: Used for continuous output (numeric values). It assigns the average value of the nearest neighbors to the new data point.\n",
    "\n",
    "Both methods rely on the concept of proximity in feature space but differ in the nature of their outputs and the specific task they are designed to perform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126b775e-aca8-4346-b60f-afa4e23bb1b1",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?\n",
    "\n",
    "Measuring the performance of K-Nearest Neighbors (KNN) depends on whether you are using KNN for classification or regression. Here are the common performance metrics for each case:\n",
    "\n",
    "### KNN Classifier\n",
    "\n",
    "1. **Accuracy**:\n",
    "   - **Definition**: The ratio of correctly predicted instances to the total instances.\n",
    "   - **Formula**: \\(\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\\)\n",
    "\n",
    "2. **Confusion Matrix**:\n",
    "   - A table that shows the true positive, true negative, false positive, and false negative predictions, providing a detailed breakdown of classification performance.\n",
    "\n",
    "3. **Precision, Recall, and F1-Score**:\n",
    "   - **Precision**: The ratio of correctly predicted positive observations to the total predicted positives.\n",
    "     - \\(\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}\\)\n",
    "   - **Recall (Sensitivity)**: The ratio of correctly predicted positive observations to all the actual positives.\n",
    "     - \\(\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}\\)\n",
    "   - **F1-Score**: The harmonic mean of precision and recall, providing a balance between the two.\n",
    "     - \\(\\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision + Recall}}\\)\n",
    "\n",
    "4. **ROC-AUC (Receiver Operating Characteristic - Area Under Curve)**:\n",
    "   - A plot of the true positive rate against the false positive rate at various threshold settings. The AUC measures the area under the ROC curve.\n",
    "\n",
    "### KNN Regressor\n",
    "\n",
    "1. **Mean Absolute Error (MAE)**:\n",
    "   - **Definition**: The average of the absolute differences between the predicted values and the actual values.\n",
    "   - **Formula**: \\(\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\\)\n",
    "\n",
    "2. **Mean Squared Error (MSE)**:\n",
    "   - **Definition**: The average of the squared differences between the predicted values and the actual values.\n",
    "   - **Formula**: \\(\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\)\n",
    "\n",
    "3. **Root Mean Squared Error (RMSE)**:\n",
    "   - **Definition**: The square root of the average of the squared differences between the predicted values and the actual values.\n",
    "   - **Formula**: \\(\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\\)\n",
    "\n",
    "4. **R-squared (Coefficient of Determination)**:\n",
    "   - **Definition**: A measure of how well the regression predictions approximate the real data points. An R-squared of 1 indicates that the regression predictions perfectly fit the data.\n",
    "   - **Formula**: \\(\\text{R}^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\\)\n",
    "\n",
    "### Implementation Example for KNN Classifier\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming y_test are the true labels and y_pred are the predicted labels\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, knn.predict_proba(X_test), multi_class='ovr')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "print(f'Confusion Matrix:\\n {conf_matrix}')\n",
    "print(f'ROC-AUC: {roc_auc:.2f}')\n",
    "```\n",
    "\n",
    "### Implementation Example for KNN Regressor\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Assuming y_test are the true values and y_pred are the predicted values\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'MAE: {mae:.2f}')\n",
    "print(f'MSE: {mse:.2f}')\n",
    "print(f'RMSE: {rmse:.2f}')\n",
    "print(f'R-squared: {r2:.2f}')\n",
    "```\n",
    "\n",
    "These metrics provide a comprehensive view of the performance of KNN for both classification and regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a17a33-4890-49ab-b529-93ec058ea484",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "The \"curse of dimensionality\" in K-Nearest Neighbors (KNN) refers to the various problems that arise when the number of features (dimensions) in the dataset is very high. Here's a concise explanation:\n",
    "\n",
    "1. **Distance Metrics Become Less Informative**: In high-dimensional spaces, the distances between data points become less meaningful because all points tend to be almost equidistant from each other. This makes it difficult for KNN to identify the true nearest neighbors.\n",
    "\n",
    "2. **Sparsity**: High-dimensional data tends to be sparse, meaning that data points are spread out across a large volume. This sparsity makes it harder to find dense regions of data and can lead to poor performance of the KNN algorithm.\n",
    "\n",
    "3. **Increased Computational Complexity**: As the number of dimensions increases, the computational cost of calculating distances also increases, leading to longer processing times.\n",
    "\n",
    "4. **Overfitting Risk**: With many dimensions, KNN can become overly sensitive to noise in the data, which can lead to overfitting.\n",
    "\n",
    "### Summary\n",
    "The curse of dimensionality makes KNN less effective and efficient in high-dimensional spaces due to less informative distances, sparsity, increased computational cost, and higher risk of overfitting. Dimensionality reduction techniques like PCA or feature selection can help mitigate these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24aa3c2-d53a-4fca-bfec-a83471b154a9",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?\n",
    "\n",
    "Handling missing values in K-Nearest Neighbors (KNN) can be done using the following methods:\n",
    "\n",
    "1. **Imputation**:\n",
    "   - **Mean/Median Imputation**: Replace missing numerical values with the mean or median of the column.\n",
    "   - **Mode Imputation**: Replace missing categorical values with the mode (most frequent value) of the column.\n",
    "\n",
    "2. **KNN Imputation**:\n",
    "   - Use a KNN-based imputer to fill in missing values. This method replaces missing values with a weighted average of the nearest neighbors' values for numerical features, or the most frequent category for categorical features.\n",
    "   - Implementation in Python can be done using `KNNImputer` from `sklearn.impute`.\n",
    "\n",
    "### Example: KNN Imputation in Python\n",
    "\n",
    "```python\n",
    "from sklearn.impute import KNNImputer\n",
    "import numpy as np\n",
    "\n",
    "# Example data with missing values\n",
    "data = np.array([[1, 2, np.nan], [3, 4, 3], [7, 6, 8], [np.nan, 5, 9]])\n",
    "\n",
    "# Initialize KNNImputer with desired number of neighbors\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "\n",
    "# Perform imputation\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "\n",
    "print(imputed_data)\n",
    "```\n",
    "\n",
    "### Summary\n",
    "- **Simple Imputation**: Replace missing values with mean, median, or mode.\n",
    "- **KNN Imputation**: Use the KNNImputer to fill in missing values based on the nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a4b39-593f-4751-95b5-d46f1e9cdefa",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?\n",
    "\n",
    "The K-Nearest Neighbors (KNN) algorithm can be used for both classification and regression tasks. Here’s a comparison of their performance and suitability for different types of problems:\n",
    "\n",
    "### KNN Classifier\n",
    "\n",
    "1. **Performance Metrics**:\n",
    "   - **Accuracy**: Measures the proportion of correct predictions.\n",
    "   - **Precision, Recall, F1-Score**: Useful for imbalanced datasets.\n",
    "   - **ROC-AUC**: Evaluates the classifier's performance across different thresholds.\n",
    "\n",
    "2. **Use Cases**:\n",
    "   - **Categorical Outcomes**: Best suited for problems where the output is a class label, such as image recognition, spam detection, and medical diagnosis.\n",
    "   - **Multi-class Classification**: Handles multiple classes well by considering the majority vote among the nearest neighbors.\n",
    "\n",
    "3. **Advantages**:\n",
    "   - Simple and intuitive.\n",
    "   - No training phase, making it easy to implement for small datasets.\n",
    "\n",
    "4. **Disadvantages**:\n",
    "   - Computationally expensive with large datasets.\n",
    "   - Sensitive to irrelevant or redundant features.\n",
    "   - Performance can degrade with high-dimensional data (curse of dimensionality).\n",
    "\n",
    "### KNN Regressor\n",
    "\n",
    "1. **Performance Metrics**:\n",
    "   - **Mean Absolute Error (MAE)**: Measures the average magnitude of errors.\n",
    "   - **Mean Squared Error (MSE)**: Measures the average of the squared differences between predicted and actual values.\n",
    "   - **Root Mean Squared Error (RMSE)**: Provides error magnitude in the same units as the target variable.\n",
    "   - **R-squared**: Indicates how well the model explains the variance in the target variable.\n",
    "\n",
    "2. **Use Cases**:\n",
    "   - **Continuous Outcomes**: Best suited for problems where the output is a continuous value, such as predicting house prices, stock prices, or temperature.\n",
    "\n",
    "3. **Advantages**:\n",
    "   - Simple to understand and implement.\n",
    "   - No assumptions about the data distribution.\n",
    "\n",
    "4. **Disadvantages**:\n",
    "   - Computationally expensive with large datasets.\n",
    "   - Sensitive to irrelevant or redundant features.\n",
    "   - Performance can degrade with high-dimensional data.\n",
    "\n",
    "### Which One is Better for Which Type of Problem?\n",
    "\n",
    "- **KNN Classifier**: \n",
    "  - Best for problems with categorical outcomes.\n",
    "  - Suitable for tasks where the decision boundary is complex and non-linear.\n",
    "  - Effective for small to medium-sized datasets with well-defined class boundaries.\n",
    "\n",
    "- **KNN Regressor**:\n",
    "  - Best for problems with continuous outcomes.\n",
    "  - Suitable for tasks where a simple, instance-based prediction method is needed.\n",
    "  - Effective for small to medium-sized datasets where the relationship between features and the target is local and non-linear.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **KNN Classifier**: Use for classification tasks with categorical outputs.\n",
    "- **KNN Regressor**: Use for regression tasks with continuous outputs.\n",
    "\n",
    "Both versions of KNN benefit from careful feature selection, distance metric choices, and parameter tuning to achieve optimal performance. The choice between them depends on the nature of the target variable in your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76444c3-c654-4234-acb1-ea93fa81bcd3",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?\n",
    "\n",
    "### Strengths and Weaknesses of KNN Algorithm\n",
    "\n",
    "#### **KNN Classifier**\n",
    "\n",
    "**Strengths**:\n",
    "1. **Simple and Intuitive**: Easy to understand and implement.\n",
    "2. **No Training Phase**: Instantaneous model creation as it stores the entire dataset.\n",
    "3. **Flexibility**: Can handle complex decision boundaries due to its non-parametric nature.\n",
    "\n",
    "**Weaknesses**:\n",
    "1. **Computational Complexity**: Slow during prediction, especially with large datasets.\n",
    "2. **High Memory Usage**: Requires storing the entire training dataset.\n",
    "3. **Curse of Dimensionality**: Performance degrades with high-dimensional data due to less informative distance metrics.\n",
    "4. **Sensitive to Noise**: Outliers and irrelevant features can adversely affect performance.\n",
    "\n",
    "**Addressing Weaknesses**:\n",
    "1. **Dimensionality Reduction**: Use techniques like PCA to reduce feature space.\n",
    "2. **Feature Scaling**: Normalize or standardize features to ensure distance metrics are meaningful.\n",
    "3. **Data Pruning**: Use techniques like KD-Trees or Ball Trees to speed up nearest neighbor search.\n",
    "4. **Feature Selection**: Remove irrelevant features and handle outliers effectively.\n",
    "\n",
    "#### **KNN Regressor**\n",
    "\n",
    "**Strengths**:\n",
    "1. **Simplicity**: Easy to implement and understand.\n",
    "2. **Flexibility**: Can model non-linear relationships between features and target values.\n",
    "3. **No Assumptions**: Does not assume a specific form of the data distribution.\n",
    "\n",
    "**Weaknesses**:\n",
    "1. **Computational Complexity**: Like classification, it’s slow during prediction with large datasets.\n",
    "2. **Curse of Dimensionality**: Similar to classification, performance suffers in high-dimensional spaces.\n",
    "3. **Sensitive to Noise**: Predictions can be skewed by outliers and noisy data.\n",
    "\n",
    "**Addressing Weaknesses**:\n",
    "1. **Dimensionality Reduction**: Apply PCA or other techniques to mitigate the curse of dimensionality.\n",
    "2. **Feature Scaling**: Ensure features are on the same scale for accurate distance calculations.\n",
    "3. **Data Pruning**: Utilize efficient data structures like KD-Trees or Ball Trees.\n",
    "4. **Robustness Techniques**: Use weighted averaging or trimming to reduce the impact of noisy data.\n",
    "\n",
    "### Summary\n",
    "\n",
    "**KNN Classifier**:\n",
    "- **Strengths**: Simple, no training phase, flexible decision boundaries.\n",
    "- **Weaknesses**: Computationally expensive, high memory usage, sensitive to high dimensions and noise.\n",
    "\n",
    "**KNN Regressor**:\n",
    "- **Strengths**: Simple, models non-linear relationships, no distribution assumptions.\n",
    "- **Weaknesses**: Computationally expensive, affected by high dimensions and noise.\n",
    "\n",
    "Both types of KNN can benefit from dimensionality reduction, feature scaling, and efficient data structures to address their weaknesses and improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74627f6-809a-4ed0-bb80-afd23b54b8b0",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "**Euclidean Distance** and **Manhattan Distance** are two common metrics used to measure the distance between data points in K-Nearest Neighbors (KNN). Here’s a brief comparison:\n",
    "\n",
    "### **Euclidean Distance**\n",
    "- **Definition**: Measures the straight-line distance between two points in a Euclidean space.\n",
    "- **Formula**: \n",
    "  \\[\n",
    "  d_{\\text{Euclidean}} = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\n",
    "  \\]\n",
    "  where \\( x_i \\) and \\( y_i \\) are the coordinates of the points, and \\( n \\) is the number of dimensions.\n",
    "- **Use Case**: Suitable for scenarios where the geometry of the space is more naturally represented by straight-line distances, such as in continuous feature spaces.\n",
    "- **Sensitivity**: Sensitive to the magnitude of differences between features.\n",
    "\n",
    "### **Manhattan Distance**\n",
    "- **Definition**: Measures the distance between two points along axes at right angles (the sum of the absolute differences).\n",
    "- **Formula**: \n",
    "  \\[\n",
    "  d_{\\text{Manhattan}} = \\sum_{i=1}^{n} |x_i - y_i|\n",
    "  \\]\n",
    "  where \\( x_i \\) and \\( y_i \\) are the coordinates of the points, and \\( n \\) is the number of dimensions.\n",
    "- **Use Case**: Suitable for grid-like data or scenarios where movement is restricted to horizontal and vertical directions, such as in urban planning or discrete feature spaces.\n",
    "- **Sensitivity**: Less sensitive to the magnitude of differences compared to Euclidean distance.\n",
    "\n",
    "### Summary\n",
    "- **Euclidean Distance**: Measures the shortest straight-line distance between points. It’s useful for continuous data where a straight-line relationship makes sense.\n",
    "- **Manhattan Distance**: Measures the distance based on a grid-like path. It’s useful for data with discrete features or where movement is restricted to orthogonal directions.\n",
    "\n",
    "The choice between Euclidean and Manhattan distance depends on the nature of the data and the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36db2802-317f-4d90-b768-23e2a155ddad",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?\n",
    "\n",
    "**Feature scaling** is crucial in K-Nearest Neighbors (KNN) for the following reasons:\n",
    "\n",
    "1. **Equal Weighting**: KNN relies on distance metrics (like Euclidean distance) to determine similarity. Features with larger scales (e.g., income in thousands vs. age in years) can disproportionately influence the distance calculations. Scaling ensures that all features contribute equally.\n",
    "\n",
    "2. **Improved Accuracy**: Without scaling, features with larger ranges can dominate the distance measure, leading to biased or inaccurate predictions. Scaling normalizes feature ranges, improving the algorithm's performance.\n",
    "\n",
    "3. **Faster Convergence**: Scaling can also help the algorithm converge faster by ensuring that distance calculations are consistent across features.\n",
    "\n",
    "### Common Scaling Techniques:\n",
    "- **Standardization**: Subtract the mean and divide by the standard deviation (\\(z\\)-score normalization).\n",
    "- **Min-Max Scaling**: Rescale features to a fixed range, usually [0, 1].\n",
    "\n",
    "In summary, feature scaling in KNN ensures that all features contribute equally to distance computations, leading to more accurate and balanced predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03b9bee-5596-4875-9cdd-db97d6e03652",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
