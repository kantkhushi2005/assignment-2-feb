{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae607340-d2f2-4717-bca5-f0c65484f335",
   "metadata": {},
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?\n",
    "\n",
    "The mathematical formula for a linear Support Vector Machine (SVM) can be expressed as follows:\n",
    "\n",
    "1. **Objective Function:**\n",
    "   Minimize the cost function:\n",
    "   \\[\n",
    "   \\frac{1}{2} \\| \\mathbf{w} \\|^2 + C \\sum_{i=1}^{N} \\xi_i\n",
    "   \\]\n",
    "   where:\n",
    "   - \\(\\mathbf{w}\\) is the weight vector.\n",
    "   - \\(C\\) is the regularization parameter.\n",
    "   - \\(\\xi_i\\) are the slack variables representing the margin violations.\n",
    "\n",
    "2. **Constraints:**\n",
    "   For each training example \\((\\mathbf{x}_i, y_i)\\), where \\(y_i \\in \\{+1, -1\\}\\) (the class label), the constraints are:\n",
    "   \\[\n",
    "   y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 - \\xi_i\n",
    "   \\]\n",
    "   where:\n",
    "   - \\(\\mathbf{x}_i\\) is the feature vector.\n",
    "   - \\(b\\) is the bias term.\n",
    "\n",
    "In summary, the linear SVM aims to find the weight vector \\(\\mathbf{w}\\) and bias \\(b\\) that maximize the margin between the two classes while minimizing classification errors and margin violations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ea5328-6e68-41ca-86c4-910cfea5d589",
   "metadata": {},
   "source": [
    "Q2. What is the objective function of a linear SVM?\n",
    "\n",
    "The objective function of a linear Support Vector Machine (SVM) is:\n",
    "\n",
    "\\[\n",
    "\\text{Minimize } \\frac{1}{2} \\| \\mathbf{w} \\|^2 + C \\sum_{i=1}^{N} \\xi_i\n",
    "\\]\n",
    "\n",
    "where:\n",
    "\n",
    "- \\(\\mathbf{w}\\) is the weight vector.\n",
    "- \\(C\\) is the regularization parameter.\n",
    "- \\(\\xi_i\\) are the slack variables representing margin violations.\n",
    "\n",
    "This function aims to find the weight vector \\(\\mathbf{w}\\) and bias \\(b\\) that maximize the margin between classes while minimizing classification errors and margin violations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8808f1a6-d841-4e41-b9eb-84ae41104a68",
   "metadata": {},
   "source": [
    "Q3. What is the kernel trick in SVM?\n",
    "\n",
    "The kernel trick in Support Vector Machines (SVM) is a technique used to handle non-linearly separable data. It involves using a kernel function to implicitly map the original feature space into a higher-dimensional space where the data may be linearly separable, without explicitly computing the coordinates in that higher-dimensional space.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "- **Kernel Function:** A function \\( K(\\mathbf{x}, \\mathbf{x}') \\) computes the dot product in the higher-dimensional space directly from the original features \\(\\mathbf{x}\\) and \\(\\mathbf{x}'\\).\n",
    "- **Common Kernels:** Examples include polynomial kernels \\( K(\\mathbf{x}, \\mathbf{x}') = (\\mathbf{x} \\cdot \\mathbf{x}' + c)^d \\) and radial basis function (RBF) kernels \\( K(\\mathbf{x}, \\mathbf{x}') = \\exp(-\\gamma \\| \\mathbf{x} - \\mathbf{x}' \\|^2) \\).\n",
    "\n",
    "The kernel trick allows SVMs to fit complex decision boundaries efficiently by leveraging the properties of the kernel function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a0de9f-a929-4505-841f-ef1e56e5eca9",
   "metadata": {},
   "source": [
    "Q4. What is the role of support vectors in SVM Explain with example\n",
    "\n",
    "Support vectors are the data points that lie closest to the decision boundary in a Support Vector Machine (SVM) model. They play a crucial role in defining the position and orientation of the hyperplane that separates different classes.\n",
    "\n",
    "**Role of Support Vectors:**\n",
    "\n",
    "1. **Defining the Margin:** Support vectors are the points that are on the edges of the margin or closest to it. They are used to determine the maximum-margin hyperplane.\n",
    "2. **Influencing the Model:** The position of the hyperplane is adjusted based on these support vectors. Removing support vectors would change the position of the hyperplane, affecting the modelâ€™s decision boundary.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "In a 2D classification problem where two classes are separated by a line, the support vectors are the data points that are closest to the line but still correctly classified. These points are critical in determining the position of the line (hyperplane). The distance between these support vectors and the line defines the margin, and the SVM aims to maximize this margin.\n",
    "\n",
    "In summary, support vectors are essential for constructing the optimal decision boundary in SVMs and directly influence the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8136e0-e391-4b3c-a99d-acf693938360",
   "metadata": {},
   "source": [
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?\n",
    "\n",
    "Here's a brief illustration of key concepts in SVM, with a description of each:\n",
    "\n",
    "1. **Hyperplane:**\n",
    "   - **Definition:** The decision boundary that separates different classes in the feature space.\n",
    "   - **Example:** In a 2D space, a hyperplane is a line that separates two classes.\n",
    "   - **Graph:** A line (in 2D) or a plane (in 3D) that divides the feature space into two regions.\n",
    "\n",
    "2. **Margin Plane:**\n",
    "   - **Definition:** Planes parallel to the hyperplane that are at the distance of the margin from it. These planes define the boundary of the margin.\n",
    "   - **Example:** For a linear SVM, there are two margins (one for each class) that are equidistant from the hyperplane.\n",
    "   - **Graph:** Lines (or planes) parallel to the hyperplane, one on each side, marking the boundary of the margin.\n",
    "\n",
    "3. **Soft Margin:**\n",
    "   - **Definition:** Allows some misclassifications or margin violations to enable a better fit on data that is not linearly separable.\n",
    "   - **Example:** A soft margin SVM will tolerate some data points being within or outside the margin.\n",
    "   - **Graph:** The hyperplane and margin planes are adjusted to allow some data points to be inside or outside the margin.\n",
    "\n",
    "4. **Hard Margin:**\n",
    "   - **Definition:** Enforces that all data points are correctly classified with no margin violations. Used when data is perfectly linearly separable.\n",
    "   - **Example:** All data points lie outside the margin planes, and no points are allowed to be within the margin.\n",
    "   - **Graph:** The hyperplane and margin planes are positioned so that all data points are correctly classified and lie outside the margin.\n",
    "\n",
    "These illustrations help to visualize how SVMs separate classes with different approaches depending on whether the data is linearly separable or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7315f0e1-6d6f-4930-90e1-bda8bbcee94d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
