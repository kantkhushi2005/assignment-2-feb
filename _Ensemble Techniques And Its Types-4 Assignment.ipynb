{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8739e8db-2b5e-43ae-9f84-ecaf6536a827",
   "metadata": {},
   "source": [
    "Q1. Preprocess the dataset by handling missing values, encoding categorical variables, and scaling the\n",
    "numerical features if necessary.\n",
    "\n",
    "To build a Random Forest classifier for predicting the risk of heart disease, follow these steps:\n",
    "\n",
    "### 1. **Preprocess the Dataset**\n",
    "\n",
    "#### a. **Load the Dataset**\n",
    "Assume you have the dataset in a CSV file named `heart_disease.csv`.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('heart_disease.csv')\n",
    "```\n",
    "\n",
    "#### b. **Handle Missing Values**\n",
    "Check for missing values and handle them appropriately.\n",
    "\n",
    "```python\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Impute or drop missing values as needed\n",
    "# For simplicity, we will drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "```\n",
    "\n",
    "#### c. **Encode Categorical Variables**\n",
    "Convert categorical variables to numeric using one-hot encoding or label encoding.\n",
    "\n",
    "```python\n",
    "# Convert categorical variables to dummy variables\n",
    "df = pd.get_dummies(df, columns=['chest_pain_type'], drop_first=True)\n",
    "```\n",
    "\n",
    "#### d. **Scale Numerical Features (if necessary)**\n",
    "Standardize numerical features if they vary significantly in scale.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "### 2. **Build the Random Forest Classifier**\n",
    "\n",
    "#### a. **Import Libraries**\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "```\n",
    "\n",
    "#### b. **Split the Dataset**\n",
    "\n",
    "```python\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "```\n",
    "\n",
    "#### c. **Train the Random Forest Classifier**\n",
    "\n",
    "```python\n",
    "# Initialize and train the Random Forest model\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "#### d. **Make Predictions and Evaluate the Model**\n",
    "\n",
    "```python\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "print('Classification Report:')\n",
    "print(class_report)\n",
    "```\n",
    "\n",
    "### Summary\n",
    "1. **Load and preprocess the dataset** by handling missing values, encoding categorical variables, and scaling numerical features.\n",
    "2. **Build and train a Random Forest classifier** with the processed data.\n",
    "3. **Evaluate the model’s performance** using metrics like accuracy, confusion matrix, and classification report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff17a956-8314-4577-bb91-dea2dd49e0ae",
   "metadata": {},
   "source": [
    "Q2. Split the dataset into a training set (70%) and a test set (30%).\n",
    "\n",
    "To split the heart disease dataset into a training set (70%) and a test set (30%), you can use the `train_test_split` function from the `sklearn.model_selection` module. Here’s how to do it step-by-step:\n",
    "\n",
    "### Code Example\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('heart_disease.csv')\n",
    "\n",
    "# Preprocess the dataset\n",
    "# Handle missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "df = pd.get_dummies(df, columns=['chest_pain_type'], drop_first=True)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print(f'Training set shape: {X_train.shape}')\n",
    "print(f'Test set shape: {X_test.shape}')\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "1. **Load the Dataset:** Read the CSV file into a DataFrame.\n",
    "2. **Preprocess the Dataset:**\n",
    "   - Handle missing values by dropping rows with missing data.\n",
    "   - Encode categorical variables using one-hot encoding.\n",
    "3. **Separate Features and Target:** Separate the features (`X`) from the target variable (`y`).\n",
    "4. **Split the Dataset:**\n",
    "   - Use `train_test_split` to divide the data into training and test sets.\n",
    "   - Set `test_size=0.3` to allocate 30% of the data to the test set and the remaining 70% to the training set.\n",
    "   - Set `random_state=42` for reproducibility.\n",
    "5. **Print Shapes:** Output the shapes of the training and test sets to confirm the split.\n",
    "\n",
    "This code will split your dataset appropriately, ready for model training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750dd97e-85cf-4a91-9170-9eb23d399936",
   "metadata": {},
   "source": [
    "Q3. Train a random forest classifier on the training set using 100 trees and a maximum depth of 10 for each\n",
    "tree. Use the default values for other hyperparameters.\n",
    "\n",
    "To train a Random Forest classifier with 100 trees and a maximum depth of 10 for each tree, follow these steps:\n",
    "\n",
    "### Code Example\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "\n",
    "# Train the model on the training set\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Print confirmation\n",
    "print(\"Random Forest model trained with 100 trees and a maximum depth of 10.\")\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "1. **Initialize the Random Forest Classifier:**\n",
    "   - `n_estimators=100` specifies the number of trees in the forest.\n",
    "   - `max_depth=10` sets the maximum depth of each tree.\n",
    "   - `random_state=42` ensures that the results are reproducible.\n",
    "\n",
    "2. **Train the Model:**\n",
    "   - Use the `.fit()` method to train the model on the training data (`X_train` and `y_train`).\n",
    "\n",
    "3. **Print Confirmation:**\n",
    "   - Output a message to confirm that the model has been trained.\n",
    "\n",
    "This code will train a Random Forest classifier with the specified parameters, ready for evaluation and further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caed92e-653b-4287-b951-8a54808fdcf7",
   "metadata": {},
   "source": [
    "Q4. Evaluate the performance of the model on the test set using accuracy, precision, recall, and F1 score.\n",
    "\n",
    "To evaluate the performance of the trained Random Forest model on the test set, you can use accuracy, precision, recall, and F1 score. Here’s how to do it:\n",
    "\n",
    "### Code Example\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print performance metrics\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "\n",
    "# Print confusion matrix and classification report\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "1. **Make Predictions:**\n",
    "   - Use the `.predict()` method to generate predictions for the test set (`X_test`).\n",
    "\n",
    "2. **Calculate Performance Metrics:**\n",
    "   - `accuracy_score(y_test, y_pred)`: Measures the proportion of correctly classified instances.\n",
    "   - `precision_score(y_test, y_pred)`: Measures the proportion of true positives among the predicted positives.\n",
    "   - `recall_score(y_test, y_pred)`: Measures the proportion of true positives among the actual positives.\n",
    "   - `f1_score(y_test, y_pred)`: Provides the harmonic mean of precision and recall, balancing the two metrics.\n",
    "\n",
    "3. **Print Confusion Matrix and Classification Report:**\n",
    "   - `confusion_matrix(y_test, y_pred)`: Shows the number of true positives, true negatives, false positives, and false negatives.\n",
    "   - `classification_report(y_test, y_pred)`: Provides a summary of precision, recall, and F1 score for each class.\n",
    "\n",
    "This code evaluates the performance of the Random Forest model using various metrics and provides a detailed analysis of how well the model performs on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a9ca34-adae-416c-9459-526ddddcda0c",
   "metadata": {},
   "source": [
    "Q5. Use the feature importance scores to identify the top 5 most important features in predicting heart\n",
    "disease risk. Visualise the feature importances using a bar chart.\n",
    "\n",
    "To identify and visualize the top 5 most important features in predicting heart disease risk using the Random Forest model, follow these steps:\n",
    "\n",
    "### Code Example\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get feature importance scores from the trained model\n",
    "importances = rf_clf.feature_importances_\n",
    "\n",
    "# Create a DataFrame with feature names and their importance scores\n",
    "features = X.columns\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Select the top 5 most important features\n",
    "top_features = feature_importances.head(5)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_features['Feature'], top_features['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 5 Most Important Features for Predicting Heart Disease Risk')\n",
    "plt.gca().invert_yaxis()  # Highest importance on top\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "1. **Extract Feature Importances:**\n",
    "   - Use `rf_clf.feature_importances_` to get the importance scores of each feature from the trained Random Forest model.\n",
    "\n",
    "2. **Create a DataFrame:**\n",
    "   - Create a DataFrame with feature names and their corresponding importance scores.\n",
    "   - Sort the DataFrame by importance scores in descending order.\n",
    "\n",
    "3. **Select Top 5 Features:**\n",
    "   - Use `head(5)` to select the top 5 most important features.\n",
    "\n",
    "4. **Visualize Using a Bar Chart:**\n",
    "   - Plot the feature importances using a horizontal bar chart with `plt.barh()`.\n",
    "   - Add labels, a title, and invert the y-axis to display the highest importance on top.\n",
    "\n",
    "This code will help you identify and visualize the most important features for predicting heart disease risk, allowing you to understand which factors are most influential in your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dfa868-4a33-4391-abd7-e3800608ec2c",
   "metadata": {},
   "source": [
    "Q6. Tune the hyperparameters of the random forest classifier using grid search or random search. Try\n",
    "different values of the number of trees, maximum depth, minimum samples split, and minimum samples\n",
    "leaf. Use 5-fold cross-validation to evaluate the performance of each set of hyperparameters.\n",
    "\n",
    "To tune the hyperparameters of the Random Forest classifier using Grid Search or Random Search and evaluate performance with 5-fold cross-validation, you can follow these steps:\n",
    "\n",
    "### Using Grid Search\n",
    "\n",
    "1. **Import Necessary Libraries**\n",
    "2. **Define Parameter Grid**\n",
    "3. **Perform Grid Search**\n",
    "4. **Evaluate the Best Model**\n",
    "\n",
    "### Code Example\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define the parameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200],         # Number of trees\n",
    "    'max_depth': [None, 10, 20, 30],             # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],              # Minimum samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4]                 # Minimum samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize Grid Search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf_clf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit Grid Search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_rf_clf = grid_search.best_estimator_\n",
    "y_pred = best_rf_clf.predict(X_test)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "### Using Random Search\n",
    "\n",
    "Alternatively, you can use Random Search for hyperparameter tuning if you want a more randomized approach:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Define the parameter distributions for Random Search\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 200),                # Number of trees\n",
    "    'max_depth': [None, 10, 20, 30, 40],             # Maximum depth of the tree\n",
    "    'min_samples_split': randint(2, 10),              # Minimum samples required to split an internal node\n",
    "    'min_samples_leaf': randint(1, 5)                 # Minimum samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Initialize Random Search with 5-fold cross-validation\n",
    "random_search = RandomizedSearchCV(estimator=rf_clf, param_distributions=param_dist, n_iter=100, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "\n",
    "# Fit Random Search to the data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_rf_clf = random_search.best_estimator_\n",
    "y_pred = best_rf_clf.predict(X_test)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "1. **Define Parameter Grid/Distribution:**\n",
    "   - For Grid Search, specify all possible values for each hyperparameter.\n",
    "   - For Random Search, define distributions from which to sample values.\n",
    "\n",
    "2. **Initialize Search:**\n",
    "   - Use `GridSearchCV` or `RandomizedSearchCV` with the defined parameter grid/distribution and 5-fold cross-validation.\n",
    "\n",
    "3. **Fit Search to Data:**\n",
    "   - Perform the search on the training data to find the best hyperparameters.\n",
    "\n",
    "4. **Evaluate the Best Model:**\n",
    "   - Retrieve the best model from the search and evaluate its performance on the test set.\n",
    "\n",
    "This process will help you find the optimal hyperparameters for your Random Forest model and assess its performance with 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e77d429-59ab-402c-8588-7ebf101f4ed5",
   "metadata": {},
   "source": [
    "Q7. Report the best set of hyperparameters found by the search and the corresponding performance\n",
    "metrics. Compare the performance of the tuned model with the default model.\n",
    "\n",
    "To report the best set of hyperparameters and corresponding performance metrics, and compare the performance of the tuned model with the default model, follow these steps:\n",
    "\n",
    "### Code Example\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the Random Forest model with default hyperparameters\n",
    "default_rf_clf = RandomForestClassifier(random_state=42)\n",
    "default_rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions with the default model\n",
    "y_pred_default = default_rf_clf.predict(X_test)\n",
    "\n",
    "# Print performance metrics for the default model\n",
    "print(\"Default Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_default):.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_default))\n",
    "\n",
    "# Define the parameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize Grid Search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42),\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5,\n",
    "                           scoring='accuracy',\n",
    "                           n_jobs=-1)\n",
    "\n",
    "# Fit Grid Search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_rf_clf = grid_search.best_estimator_\n",
    "y_pred_best = best_rf_clf.predict(X_test)\n",
    "\n",
    "# Print the best parameters and performance metrics for the tuned model\n",
    "print(\"Tuned Model Performance:\")\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Cross-Validation Score: {best_score:.2f}\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_best):.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_best))\n",
    "\n",
    "# Alternatively, you can use RandomizedSearchCV for hyperparameter tuning\n",
    "# Define the parameter distributions for Random Search\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_depth': [None, 10, 20, 30, 40],\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 5)\n",
    "}\n",
    "\n",
    "# Initialize Random Search with 5-fold cross-validation\n",
    "random_search = RandomizedSearchCV(estimator=RandomForestClassifier(random_state=42),\n",
    "                                    param_distributions=param_dist,\n",
    "                                    n_iter=100,\n",
    "                                    cv=5,\n",
    "                                    scoring='accuracy',\n",
    "                                    n_jobs=-1,\n",
    "                                    random_state=42)\n",
    "\n",
    "# Fit Random Search to the data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score from Random Search\n",
    "best_params_random = random_search.best_params_\n",
    "best_score_random = random_search.best_score_\n",
    "\n",
    "# Evaluate the best model from Random Search on the test set\n",
    "best_rf_clf_random = random_search.best_estimator_\n",
    "y_pred_best_random = best_rf_clf_random.predict(X_test)\n",
    "\n",
    "# Print the best parameters and performance metrics for the Random Search tuned model\n",
    "print(\"Random Search Tuned Model Performance:\")\n",
    "print(f\"Best Parameters: {best_params_random}\")\n",
    "print(f\"Best Cross-Validation Score: {best_score_random:.2f}\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_best_random):.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_best_random))\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "1. **Default Model:**\n",
    "   - Train a Random Forest classifier with default hyperparameters.\n",
    "   - Evaluate and report its accuracy and classification metrics on the test set.\n",
    "\n",
    "2. **Hyperparameter Tuning with Grid Search:**\n",
    "   - Perform Grid Search to find the best hyperparameters.\n",
    "   - Print the best parameters and cross-validation score from Grid Search.\n",
    "   - Evaluate the performance of the tuned model on the test set.\n",
    "\n",
    "3. **Comparison:**\n",
    "   - Compare the performance metrics (accuracy, precision, recall, F1 score) of the tuned model with the default model.\n",
    "   - Optionally, perform a Random Search to compare results and find the best hyperparameters.\n",
    "\n",
    "This approach allows you to see how hyperparameter tuning improves model performance compared to using default settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a772484-e5c2-4979-a231-b2e96d9f59b7",
   "metadata": {},
   "source": [
    "Q8. Interpret the model by analysing the decision boundaries of the random forest classifier. Plot the\n",
    "decision boundaries on a scatter plot of two of the most important features. Discuss the insights and\n",
    "limitations of the model for predicting heart disease risk.\n",
    "\n",
    "To analyze the decision boundaries of a Random Forest classifier, you can plot them on a scatter plot using two of the most important features. Here's how to interpret the model, plot the decision boundaries, and discuss the insights and limitations:\n",
    "\n",
    "### Steps to Plot Decision Boundaries\n",
    "\n",
    "1. **Identify the Two Most Important Features**\n",
    "2. **Create a Mesh Grid for Decision Boundary Plotting**\n",
    "3. **Plot the Decision Boundaries and Scatter Plot**\n",
    "\n",
    "### Code Example\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the two most important features\n",
    "top_features = feature_importances.head(2)\n",
    "feature_names = top_features['Feature'].values\n",
    "X_top_features = X[feature_names]\n",
    "\n",
    "# Train a Random Forest model on the reduced feature set\n",
    "rf_clf_reduced = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_clf_reduced.fit(X_top_features, y)\n",
    "\n",
    "# Create a mesh grid for plotting decision boundaries\n",
    "x_min, x_max = X_top_features[feature_names[0]].min() - 1, X_top_features[feature_names[0]].max() + 1\n",
    "y_min, y_max = X_top_features[feature_names[1]].min() - 1, X_top_features[feature_names[1]].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "# Predict on the mesh grid\n",
    "Z = rf_clf_reduced.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundaries\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "\n",
    "# Plot the scatter plot of the two most important features\n",
    "plt.scatter(X_top_features[feature_names[0]], X_top_features[feature_names[1]], c=y, edgecolors='k', cmap='coolwarm')\n",
    "plt.xlabel(feature_names[0])\n",
    "plt.ylabel(feature_names[1])\n",
    "plt.title('Decision Boundaries and Scatter Plot of Top 2 Features')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "1. **Identify Features:**\n",
    "   - Use the feature importance scores to select the top 2 most important features.\n",
    "\n",
    "2. **Train Reduced Model:**\n",
    "   - Train a Random Forest classifier using only the two selected features to simplify visualization.\n",
    "\n",
    "3. **Create Mesh Grid:**\n",
    "   - Define a mesh grid covering the range of the two features to plot the decision boundaries.\n",
    "\n",
    "4. **Predict and Plot:**\n",
    "   - Use the model to predict class labels over the mesh grid and plot the decision boundaries.\n",
    "   - Overlay the scatter plot of the two features to visualize how the decision boundaries separate the classes.\n",
    "\n",
    "### Insights and Limitations\n",
    "\n",
    "**Insights:**\n",
    "- **Decision Boundaries:**\n",
    "  - The plot shows how the Random Forest classifier separates different classes based on the two most important features. This visualization helps in understanding the regions where the model predicts different classes.\n",
    "- **Feature Importance:**\n",
    "  - By focusing on the top features, you can see which features contribute most to decision-making.\n",
    "\n",
    "**Limitations:**\n",
    "- **Limited to Two Features:**\n",
    "  - This analysis only considers two features, whereas the model uses all features for prediction. Important interactions between other features are not visible in this plot.\n",
    "- **Overfitting Risk:**\n",
    "  - Random Forests are generally robust, but with a large number of trees and high depth, there is still a risk of overfitting to the training data.\n",
    "- **Interpretability:**\n",
    "  - Decision boundaries provide an intuition but may not fully capture the complexity of interactions in higher-dimensional space.\n",
    "\n",
    "This approach gives a visual understanding of how the model separates classes based on selected features but should be complemented with other metrics and analysis for a comprehensive evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e892ee3e-8b6f-46d2-9618-3f4400ce20da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
